(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{CHYI:function(n,e,a){"use strict";a.d(e,"a",(function(){return r}));a("HQhv");var i=["Enero","Febrero","Marzo","April","Mayo","Junio","Julio","Agosto","Septiembre","Octubre","Noviembre","Diciembre"];function r(n){var e;return e=n.split("-"),parseInt(e[2])+" de "+i[parseInt(e[1])-1]+", "+e[0]}},EKbp:function(n){n.exports=JSON.parse('{"data":{"site":{"siteMetadata":{"author":"Miguel Angel Carrasco","description":"Un blog con una compilación de notas en matemáticas, ciencias de la computación, programación y otros temas interesantes, usado principalmente para recordar y compartir ideas.","title":"Ideas Interesantes"}}}}')},Mtj7:function(n){n.exports=JSON.parse('{"data":{"allMdx":{"edges":[{"node":{"frontmatter":{"date":"2022-11-14T03:14:24Z","description":"Se muestra una implementación eficiente para calcular los términos de la secuencia de fibonacci en tiempo logarítmico O(log(n))","title":"Algoritmo para el cálculo de los términos de la secuencia de Fibonacci con complejidad O(log(n))"},"slug":"fib-optimization","rawBody":"---\\ntitle: Algoritmo para el cálculo de los términos de la secuencia de Fibonacci con complejidad O(log(n))\\ndate: \\"2022-11-14T03:14:24Z\\"\\ndescription: \\"Se muestra una implementación eficiente para calcular los términos de la secuencia de fibonacci en tiempo logarítmico O(log(n))\\"\\n---\\n\\nimport Fib from \'../../src/images/fibonacci.svg\';\\n\\nUn problema muy recurrente en el estudio de algoritmos es la recursividad, para la cual se suele presentar de ejemplo clásico la\\nfamosa sucesión de Fibonacci, que está definida como la sucesión de enteros donde cada término está compuesto por la suma de\\nlos dos términos anteriores, empezando por el $0$ y el $1$\\n\\n$$ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... $$\\n\\nLa cual se puede escribir como\\n\\n$$\\n\\\\begin{aligned}\\nF_0 &= 0 \\\\\\\\\\nF_1 &= 1 \\\\\\\\\\nF_n &= F_{n-1} + F_{n-2}, \\\\forall n \\\\in \\\\mathbb{N} \\\\land n > 1\\n\\\\end{aligned}\\n$$\\n\\nLa forma anterior de definir la secuencia de fibonacci resulta muy conveniente\\npara programar una función recursiva que calcule el valor de $F_n$ en algún lenguaje de programación,\\ncomo por ejemplo en lenguaje C\\n\\n```cpp\\nint fib(int n){\\n    if(n == 0){\\n        return 0;\\n    }\\n\\n    if(n == 1){\\n        return 1;\\n    }\\n    return fib(n - 1) + fib(n - 2);\\n}\\n```\\nEl gran inconveniente con esta implementación es que su complejidad algorítmica resulta ser exponencial.\\nPodemos observar que por cada llamada a esta función, la mayoría de las veces se \\"expanden\\" otras\\ndos llamadas. Por lo que su complejidad se acerca a $O(2^n)$. Por ejemplo para calcular fib(6), se tienen\\nque hacer 25 llamadas a la misma función, muchas de las cuales repiten innecesariamente el mismo cálculo.\\n\\n<Fib style=\\"width:100%\\"/>\\n\\nEn este artículo presento ideas interesantes para reducir la complejidad computacional de este problema con diferentes\\nalgoritmos.\\n\\n## Reducción a tiempo lineal $O(n)$\\n\\nUna mejora en la implementación permite bajar la complejidad a $O(n)$, es decir, resolver el problema\\nen tiempo lineal. Como por ejemplo la función de recursión de cola (tail recursion) siguiente\\n\\n```cpp\\nunsigned long fib(unsigned long n, unsigned long currentVal = 1,\\n    unsigned long previousVal = 0) {\\n    if (n == 0) {\\n        return previousVal;\\n    }\\n\\n    if (n == 1) {\\n        return currentVal;\\n    }\\n    return fib(n - 1, currentVal + previousVal, currentVal);\\n}\\n```\\nDonde básicamente se calcula la suma de todos los componentes de la sucesión hasta alcanzar el término buscado.\\nAun así queda la duda de si podría existir una implementación más eficiente. De hecho se puede reducir a un problema\\nque se resuelve en tiempo logarítmico, es decir, existe un algoritmo con complejidad $O(\\\\log(n))$.\\n\\n## Reducción a $O(log(n))$ usando la fórmula de Binet\\n\\nEn el caso de la secuencia de Fibonacci se le atribuye a [Jacques Philippe Marie Binet](https://en.wikipedia.org/w/index.php?title=Jacques_philippe_Marie_Binet&oldid=1067689697)\\nla siguiente fórmula explícita para obtener $F_n$\\n\\n$$ \\\\boxed{ F_n = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\left( \\\\frac{1+\\\\sqrt{5}}{2} \\\\right)^n - \\\\left( \\\\frac{1-\\\\sqrt{5}}{2} \\\\right)^n \\\\right] }$$\\n\\nOtra forma de escribir esta fórmula es la siguiente\\n\\n$$ \\\\boxed{ F_n = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^n - \\\\psi^n \\\\right] } $$\\n\\ndonde $\\\\varphi=\\\\frac{1+\\\\sqrt{5}}{2}$ y $\\\\psi=\\\\frac{1-\\\\sqrt{5}}{2}$\\n\\nA $\\\\varphi$ se le suele denominar la [proporción áurea](https://en.wikipedia.org/wiki/Golden_ratio).\\n\\nEscribí anteriormente un [artículo con la demostración de la fórmula de Binet mediante inducción fuerte\\ny también derivando la fórmula usando resultados del álgebra lineal](binet-formula).\\nEs importante revisar este artículo para comprender mejor las siguientes optimizaciones.\\n\\nUna implementación de la fórmula de Binet en C, podría ser la siguiente\\n\\n```cpp\\nconst long double FIVE_SQUARE = pow(5, 0.5);\\nconst long double PHI = (1 + FIVE_SQUARE) / 2;\\nconst long double PSI = (1 - FIVE_SQUARE) / 2;\\n\\nunsigned long fib(unsigned long n) {\\n    return ((pow(PHI, n) - pow(PSI, n)) / FIVE_SQUARE);\\n}\\n```\\nLa cual nos reduce la complejidad algorítmica aún más a $O(\\\\log(n))$, ya que la manera más eficiente conocida de elevar\\na la potencia $n$ tiene complejidad $O(\\\\log(n))$ usando la [exponenciación por cuadrados](https://en.wikipedia.org/wiki/Exponentiation_by_squaring).\\nEsta idea es muy importante para los siguientes algoritmos.\\n\\nEl único problema de esta implementación es que en algún punto aparecen errores de redondeo conforme aumenta $n$.\\nEn esta implementación en C, a partir de $n > 71$, aparecen errores en el resultado.\\nSin embargo, resulta bastante interesante como usando una fórmula explicita es posible reducir la complejidad a tiempo\\nlogarítmico.\\n\\n## Optimización a $O(log(n))$ sin pérdida de precisión por errores de redondeo\\nEn el algoritmo anterior que usa la fórmula de Binet, notamos que su complejidad es logarítmica, únicamente\\nporque para elevar un número a la potencia $n$, el algoritmo más eficiente es la [exponenciación por cuadrados](https://en.wikipedia.org/wiki/Exponentiation_by_squaring).\\nDe no ser por esta razón las demás operaciones se hacen idealmente, en tiempo constante $O(1)$.\\n\\nPara entender el siguiente algoritmo que conserva la complejidad logarítmica sin perder precisión, tenemos que\\nentender bien la exponenciación por cuadrados.\\n\\n### Exponenciación por cuadrados\\n\\nLa exponenciación es una operación muy importante en la teoría algorítmica de números.\\nPor ejemplo, se usa intensamente en muchas pruebas de primalidad y algoritmos de factorización.\\nPor lo tanto, se han estudiado métodos eficientes durante siglos.\\n\\nEn los criptosistemas basados en el problema del logaritmo discreto, la exponenciación suele ser la parte que\\nconsume más tiempo y, es por esta razón, que determina la eficiencia de los protocolos criptográficos\\ncomo el intercambio de llaves de Diffie-Hellman, el algoritmo ElGamal para la cripotgrafía de llave pública\\ny algoritmos para firma digital.\\n\\nla exponenciación por cuadrados consiste en que podemos simplificar el cálculo de potencias muy grandes,\\ndebido a la observación de que dado un entero positivo $n$, sabemos que\\n\\n$$\\nx^n =\\n\\\\begin{cases}\\nx(x^2)^{\\\\frac{n-1}{2}} &\\\\text{si } n \\\\text{ es impar}\\\\\\\\\\n(x^2)^{\\\\frac{n}{2}} &\\\\text{si } n \\\\text{ es par}\\n\\\\end{cases}\\n$$\\n\\nPor ejemplo si queremos calcular $2^{10}$, en lugar de hacer $10$ multiplicaciones\\n\\n$$\\n2^{10}=2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot2\\\\cdot=1024\\n$$\\n\\nPodemos hacer menos operaciones. Aplicando primeramente la regla anterior, dado que $10$ es par\\n\\n$$\\n2^{10} = (2\\\\cdot2)^{\\\\frac{10}{2}}=(4)^{5}\\n$$\\n\\nAl multiplicar $2\\\\cdot2$ y hacer la división $\\\\frac{10}{2}=5$ llevamos dos operaciones.\\nLuego volvemos a aplicar la regla anterior, pero notando que $5$ es impar\\n\\n$$\\n4^5 = 4(4\\\\cdot4)^2 = 4(16)^2 = 4(256) = 1024\\n$$\\n\\nEn este último paso hicimos primero $4\\\\cdot4=16$, luego $16\\\\cdot16=256$ y finalmente $4(256)=1024$, es decir,\\n3 operaciones más. Con lo cual reducimos el número de operaciones a 5 en lugar de 10.\\n\\nUna implementación en C recursiva de la exponenciación por cuadrados es la siguiente\\n\\n```cpp\\nint exponentiation(int a, int b){\\n    if(b==1)\\n        return a;\\n    if(b%2==1)\\n        return a*exponentiation(a, b-1);\\n    int tmp_exp = exponentiation(a, b/2)\\n    return tmp_exp * tmp_exp;\\n}\\n```\\nEs claro que el número de llamadas a esta función será de $\\\\lceil\\\\log_{2}n\\\\rceil$. Lo interesante de la exponenciación\\npor cuadrados es que no solo sirve para multiplicar números, sino que este resultado es válido para [semigrupos](https://en.wikipedia.org/wiki/Semigroup).\\n\\n**Definición** Un semigrupo es un conjunto $S$, junto con una operación binaria $\\\\cdot$, que es una función\\n$\\\\cdot:S\\\\rightarrow S \\\\times S$, que es asociativa. Es decir, para todo $a,b,c \\\\in S$ se cumple que $a\\\\cdot(b \\\\cdot c) = (a \\\\cdot b) \\\\cdot c$\\n\\nEs claro que el conjunto de matrices cuadradas de $n \\\\times n$, junto con la multiplicación matricial, forman un semigrupo.\\nLa multiplicación de matrices es cerrada y asociativa.\\n\\n> Un semigrupo puede ser visto como un [magma](https://en.wikipedia.org/wiki/Magma_(algebra)) que además es asociativo.\\n> Un [grupo](https://en.wikipedia.org/wiki/Group_(mathematics)) es una subclase de semigrupo. En particular el\\n> [grupo general lineal](https://en.wikipedia.org/wiki/General_linear_group) $GL(n,\\\\mathbb{F})$ de grado $n$ sobre el\\n> cuerpo (o campo) $\\\\mathbb{F}$, si se usa el cuerpo de los reales o complejos, puede ser visto como el grupo formado\\n> por matrices cuadradas invertibles de $n \\\\times n$ con la multiplicación de matrices. Este grupo en particular\\n> forma un [Grupo de Lie](https://en.wikipedia.org/wiki/Lie_group), ya que forma también una variedad diferencial.\\n> Los grupos de Lie son ampliamente utilizados en la matemática moderna y en la física. En la física se usan en el [modelo de partículas estandard](https://en.wikipedia.org/wiki/Standard_Model),\\n> ya que es un tipo de [teoría de campo de gauge](https://en.wikipedia.org/wiki/Gauge_theory), donde los grupos de Lie son fundamentales.\\n> Los semigrupos también son ampliamente utilizados en la teoría algebráica de autómatas finitos y en la criptografía elíptica.\\n\\nPodemos entonces usar la forma matricial de la relación de recurrencia de la secuencia de fibonacci que se usó para\\n[derivar la fórmula de binet en el artículo anterior](binet-formula)\\n\\n$$\\n\\\\begin{bmatrix}\\nF_{n+1} \\\\\\\\\\nF_{n}\\n\\\\end{bmatrix}\\n=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}^n\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n$$\\n\\nY utilizar la exponenciación por cuadrados para elevar a la potencia $n$ la matriz\\n$\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}$. Que sobre el semigrupo formado por las matrices de $2 \\\\times 2$ es también válido.\\n\\nPor lo que podemos escribir la siguiente implementación que utiliza la exponenciación por cuadrados de esta matriz\\npara calcular $F_n$ en tiempo logarítmico, es decir con complejidad computacional $O(\\\\log(n))$.\\n\\n```cpp\\n/**\\n * Multiplica la matriz a por la matrix b y deposita el resultado\\n * en la matriz r\\n *\\n * @param a matriz de 2x2 como un arreglo bidimensional\\n * @param b matriz de 2x2 como un arreglo bidimensional\\n * @param r matriz resultado de 2x2 como arreglo bidimensional\\n */\\nvoid matrixMultiply(unsigned long a[2][2], unsigned long b[2][2],\\n    unsigned long r[2][2]) {\\n\\n    unsigned long r00 = a[0][0] * b[0][0] + a[0][1] * b[1][0];\\n    unsigned long r01 = a[0][0] * b[0][1] + a[0][1] * b[1][1];\\n    unsigned long r10 = a[1][0] * b[0][0] + a[1][1] * b[1][0];\\n    unsigned long r11 = a[1][0] * b[0][1] + a[1][1] * b[1][1];\\n\\n    r[0][0] = r00;\\n    r[0][1] = r01;\\n    r[1][0] = r10;\\n    r[1][1] = r11;\\n}\\n\\n\\n/**\\n * Eleva la matriz \\"a\\" a la potencia n y deposita el resultado en \\"r\\"\\n * usando exponenciación por cuadrados\\n *\\n * @param a matriz de 2x2 como un arreglo bidimensional\\n * @param n un entero que representa la potencia a la que se elevará a\\n * @param r matriz resultado de 2x2 como arreglo bidimensional\\n * @return\\n */\\nvoid power(unsigned long a[2][2], int n, unsigned long r[2][2]) {\\n    // deposita la matriz identidad en r\\n    if (n == 0) {\\n        r[0][0] = 1;\\n        r[0][1] = 0;\\n        r[1][0] = 0;\\n        r[1][1] = 1;\\n        return;\\n    }\\n\\n    // copia a en r\\n    if (n == 1) {\\n        for (int j = 0; j < 2; j++) {\\n            for (int i = 0; i < 2; i++) {\\n                r[j][i] = a[j][i];\\n            }\\n        }\\n        return;\\n    }\\n\\n    unsigned long a2[2][2];\\n    matrixMultiply(a, a, a2);\\n\\n    if (n % 2 == 0) {\\n        power(a2, n / 2, r);\\n    } else {\\n        power(a2, (n - 1) / 2, r);\\n        matrixMultiply(a, r, r);\\n    }\\n}\\n\\nunsigned long fib(unsigned long n) {\\n    unsigned long a[2][2] = {{1, 1},\\n                             {1, 0}};\\n    unsigned long r[2][2];\\n\\n    power(a, n, r);\\n\\n    return r[1][0];\\n}\\n```\\n\\nLo interesante de esta implementación es que no sacrifica precisión y no genera errores de redondeo como en la\\nimplementación de la fórmula de Binet, con la misma complejidad algorítmica $O(\\\\log(n))$.\\n\\n## Referencias\\n\\n- [Wikipedia: Fibonacci Number](https://en.wikipedia.org/wiki/Fibonacci_number)\\n- [Wikipedia: Exponentiation by squaring](https://en.wikipedia.org/wiki/Exponentiation_by_squaring)\\n- Fibonacci and Lucas Numbers with Applications, Volume 1, 2nd Edition - Thomas Koshy\\n- Handbook of Elliptic and Hyperelliptic Curve Cryptography - Kenneth H. Rosen, Ph.D.\\n- [ICS 161: Design and Analysis of Algorithms - Lecture notes for January 9, 1996 - David Eppstein](https://www.ics.uci.edu/~eppstein/161/960109.html)\\n- [Why is the complexity of computing the Fibonacci series 2^n and not n^2?](https://stackoverflow.com/questions/7547133/why-is-the-complexity-of-computing-the-fibonacci-series-2n-and-not-n2)\\n- [The Fibonacci Sequence - Thomas Browning](https://math.berkeley.edu/~tb65536/Fibonacci_Exposition.pdf)\\n- [Program for nth fibonacci - Geek for geeks](https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/)\\n- [Fibonacci Numbers Algorithm - The GNU Multiple Precision Arithmetic Library](https://gmplib.org/manual/Fibonacci-Numbers-Algorithm)\\n"}},{"node":{"frontmatter":{"date":"2022-11-08T03:08:19Z","description":"Se demuestra y se deriva la fórmula de Binet para obtener una forma cerrada o explícita de la secuencia de fibonacci","title":"Demostracíón y derivación de la fórmula de Binet"},"slug":"binet-formula","rawBody":"---\\ntitle: Demostracíón y derivación de la fórmula de Binet\\ndate: \\"2022-11-08T03:08:19Z\\"\\ndescription: \\"Se demuestra y se deriva la fórmula de Binet para obtener una forma cerrada o explícita de la secuencia\\nde fibonacci\\"\\n---\\n\\nEn la literatura y en la web se pueden encontrar varias demostraciones para la fórmula de Binet, pero en mi experiencia\\nalgunas de estas demostraciones carecen del formalismo necesario. También hay otras demostraciones en donde el formalismo está presente,\\nsin embargo no suelen ser tan claras. En este artículo trato de balancear estas dos situaciones tratando de usar un lenguaje claro,\\ncon el suficiente formalismo, sin obviar u omitir pasos.\\n\\nLa fórmula de Binet es una fórmula explícita para obtener cualquier término de la secuencia de Fibonacci.\\n\\n## La secuencia de Fibonacci\\n\\nLa [secuencia de Fibonacci](https://en.wikipedia.org/wiki/Fibonacci_number) es una famosa sucesión de enteros\\ndonde cada término es la suma de los dos términos anteriores, empezando por el $0$ y el $1$, es decir\\n\\n$$ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ... $$\\n\\nLa cual se puede escribir como\\n\\n$$\\n\\\\begin{aligned}\\nF_0 &= 0 \\\\\\\\\\nF_1 &= 1 \\\\\\\\\\nF_n &= F_{n-1} + F_{n-2}, \\\\forall n \\\\in \\\\mathbb{N} \\\\land n > 1\\n\\\\end{aligned}\\n$$\\n\\n## La fórmula de Binet\\n\\nSe le atribuye a [Jacques Philippe Marie Binet](https://en.wikipedia.org/w/index.php?title=Jacques_philippe_Marie_Binet&oldid=1067689697)\\nla siguiente fórmula explícita para obtener $F_n$ de los términos de la secuencia de fibonacci\\n\\n$$\\\\boxed{ F_n = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\left( \\\\frac{1+\\\\sqrt{5}}{2} \\\\right)^n - \\\\left( \\\\frac{1-\\\\sqrt{5}}{2} \\\\right)^n \\\\right] }$$\\n\\nquien la encontró en 1843. Aunque de hecho, fue descubierta en 1718 por el matemático francés [Abraham De Moivre](https://en.wikipedia.org/wiki/Abraham_de_Moivre)\\n(1667–1754) usando funciones generadoras.\\nFue derivado de forma independiente en 1844 por el ingeniero y matemático francés [Gabriel Lamé](https://en.wikipedia.org/wiki/Gabriel_Lamé) (1795–1870).\\n\\nOtra forma de escribir esta fórmula es la siguiente\\n\\n$$ \\\\boxed{ F_n = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^n - \\\\psi^n \\\\right] } $$\\n\\ndonde $\\\\varphi=\\\\frac{1+\\\\sqrt{5}}{2}$ y $\\\\psi=\\\\frac{1-\\\\sqrt{5}}{2}$\\n\\nA $\\\\varphi$ se le suele denominar la proporción áurea ([Golden ratio](https://en.wikipedia.org/wiki/Golden_ratio) en inglés).\\n\\nExisten unas propiedades entre $\\\\varphi$ y $\\\\psi$ muy útiles que usaremos en las siguientes demostraciones. Estas propiedades\\nson las siguientes\\n\\n1. $\\\\varphi^{-1} = \\\\psi$\\n2. $\\\\psi^{-1} = - \\\\varphi$\\n3. $\\\\varphi = 1+\\\\varphi^{-1}$\\n4. $\\\\psi = 1+\\\\psi^{-1}$\\n\\nDemostración de estas propiedades:\\n\\n1.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\varphi^{-1} &= \\\\frac{2}{1+\\\\sqrt{5}} = \\\\frac{2(1-\\\\sqrt{5})}{(1+\\\\sqrt{5})(1-\\\\sqrt{5})} \\\\\\\\\\n&= \\\\frac{2(1-\\\\sqrt{5})}{1-5} = \\\\frac{-2(1-\\\\sqrt{5})}{4} = - \\\\frac{1-\\\\sqrt{5}}{2} = - \\\\psi\\n\\\\end{aligned}\\n$$\\n\\n2.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\psi^{-1} &= \\\\frac{2}{1-\\\\sqrt{5}} = \\\\frac{2(1+\\\\sqrt{5})}{(1-\\\\sqrt{5})(1+\\\\sqrt{5})} \\\\\\\\\\n&= \\\\frac{2(1+\\\\sqrt{5})}{1-5} = \\\\frac{-2(1+\\\\sqrt{5})}{4} = - \\\\frac{1+\\\\sqrt{5}}{2} = - \\\\varphi\\n\\\\end{aligned}\\n$$\\n\\n3.\\n\\n$$\\n1+\\\\varphi^{-1} = 1 - \\\\psi = 1 - \\\\frac{1-\\\\sqrt{5}}{2} = \\\\frac{2-1+\\\\sqrt{5}}{2} = \\\\frac{1+\\\\sqrt{5}}{2} = \\\\varphi\\n$$\\n\\n4.\\n\\n$$\\n1+\\\\psi^{-1} = 1 - \\\\varphi = 1 - \\\\frac{1+\\\\sqrt{5}}{2} = \\\\frac{2-1-\\\\sqrt{5}}{2} = \\\\frac{1-\\\\sqrt{5}}{2} = \\\\psi\\n$$\\n\\nDe donde quedan demostradas las propiedades $\\\\square$.\\n\\n## Demostración por inducción fuerte de la fórmula de Binet\\n\\nEs fácil demostrar que la fórmula de Binet se cumple para todo $n \\\\in \\\\mathbb{N} \\\\cup \\\\{0\\\\}$ usando\\n[inducción fuerte](https://en.wikipedia.org/wiki/Mathematical_induction#Complete_(strong)_induction).\\n\\nPara $n=0$ vemos que se cumple\\n\\n$$\\n\\\\begin{aligned}\\nF_0 &= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\left( \\\\frac{1+\\\\sqrt{5}}{2} \\\\right)^0 - \\\\left( \\\\frac{1-\\\\sqrt{5}}{2} \\\\right)^0 \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ 1 - 1 \\\\right] = \\\\frac{0}{\\\\sqrt{5}} = 0\\n\\\\end{aligned}\\n$$\\n\\nPara $n=1$ vemos que también se cumple\\n\\n$$\\n\\\\begin{aligned}\\nF_1 &= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\left( \\\\frac{1+\\\\sqrt{5}}{2} \\\\right)^1 - \\\\left( \\\\frac{1-\\\\sqrt{5}}{2} \\\\right)^1 \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\frac{1+\\\\sqrt{5}-1+\\\\sqrt{5}}{2} \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\frac{2\\\\sqrt{5}}{2} \\\\right] = \\\\frac{\\\\sqrt{5}}{\\\\sqrt{5}} = 1\\n\\\\end{aligned}\\n$$\\n\\nAhora supongamos que se cumple para $n=k$, $n=k-1$ y para todo entero anterior a k, mayor o igual a 0, para algún $k$.\\nVamos a demostrar que eso implica que se cumple para $n=k+1$. Por simplicidad usaremos la forma que usa a $\\\\varphi$ y\\na $\\\\psi$, de tal manera que de acuerdo a nuestra hipótesis de inducción se cumple\\n$ F_k = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k - \\\\psi^k \\\\right] $ y\\n$ F_{k-1} = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^{k-1} - \\\\psi^{k-1} \\\\right] $. Por lo que entonces\\n\\n$$\\n\\\\begin{aligned}\\nF_{k+1} &= F_{k} + F_{k-1} \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k - \\\\psi^k \\\\right] + \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^{k-1} - \\\\psi^{k-1} \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k - \\\\psi^k + \\\\varphi^{k-1} - \\\\psi^{k-1} \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k  + \\\\varphi^{k-1} - \\\\psi^k - \\\\psi^{k-1}  \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ (\\\\varphi^k  + \\\\varphi^{k-1}) - (\\\\psi^{k} + \\\\psi^{k-1}) \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k (1 + \\\\varphi^{-1}) - \\\\psi^{k} (1 + \\\\psi^{-1}) \\\\right] \\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\nY de las propiedades explicadas en la sección anterior, dado que $1 + \\\\varphi^{-1} = \\\\varphi$ y $1 + \\\\psi^{-1} = \\\\psi$,\\ntenemos que\\n\\n$$\\n\\\\begin{aligned}\\nF_{k+1} &= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k (1 + \\\\varphi^{-1}) - \\\\psi^{k} (1 + \\\\psi^{-1}) \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^k \\\\varphi - \\\\psi^{k} \\\\psi \\\\right] \\\\\\\\\\n&= \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^{k+1} - \\\\psi^{k+1} \\\\right] \\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\nQue justamente era lo que queríamos demostrar. $\\\\square$\\n\\n## Derivación de la fórmula de Binet usando álgebra lineal\\n\\nAunque hemos demostrado la fórmula de Binet usando inducción fuerte y con ello tenemos la certeza de que dicha fórmula\\nes verdadera, aún no tenemos idea de cómo fue que se obtuvo esa fórmula en primer lugar.\\nUna forma de encontrar esta fórmula es haciendo uso del álgebra lineal.\\n\\nNotemos primero que podemos escribir $F_2$ y $F_1$ en forma matricial como\\n\\n$$\\n\\\\begin{bmatrix}\\nF_2 \\\\\\\\\\nF_1\\n\\\\end{bmatrix}\\n=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n$$\\n\\nY observamos que podemos escribir también $F_3$ y $F_2$ como\\n\\n$$\\n\\\\begin{aligned}\\n\\\\begin{bmatrix}\\nF_3 \\\\\\\\\\nF_2\\n\\\\end{bmatrix}\\n&=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nF_2 \\\\\\\\\\nF_1\\n\\\\end{bmatrix}\\n\\\\\\\\\\n&=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n\\\\\\\\\\n&=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}^2\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n\\\\end{aligned}\\n$$\\n\\nEs claro que repitiendo este proceso $n$ veces podemos escribir $F_{n+1}$ y $F_{n}$ como\\n\\n$$\\n\\\\begin{bmatrix}\\nF_{n+1} \\\\\\\\\\nF_{n}\\n\\\\end{bmatrix}\\n=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}^n\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n$$\\n\\nAhora, si definimos\\n\\n$$\\nA=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n$$\\n\\nPodemos rescribir esta matriz como\\n\\n$$\\n\\\\begin{bmatrix}\\nF_{n+1} \\\\\\\\\\nF_{n}\\n\\\\end{bmatrix}\\n=\\nA^n\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix}\\n$$\\n\\nEs claro que si encontramos una forma explícita para calcular $A^n$, se obtendrá una fórmula explícita para\\n$F_{n}$ y $F_{n+1}$. Una forma de simplificar el cálculo de elevar a la potencia $n$ una matriz, es mediante un\\nproceso de diagonalización. Es decir, si podemos encontrar una matriz invertible $P$ y otra matriz $D$ tal que\\n\\n$$\\nA = PDP^{-1}\\n$$\\n\\ndonde $D$ sea una matriz diagonal, es decir de la forma\\n\\n$$\\nD = \\\\begin{bmatrix}\\n\\\\lambda_1 & 0\\\\\\\\\\n0 & \\\\lambda_2\\n\\\\end{bmatrix}\\n$$\\n\\nCalcular $A^n$ sería muy sencillo, ya que\\n\\n$$\\n\\\\begin{aligned}\\nA^n &= (PDP^{-1})^n = \\\\overbrace{(PDP^{-1}) (PDP^{-1}) \\\\cdots (PDP^{-1})}^{\\\\text{n veces}} \\\\\\\\\\n&= PD(P^{-1}P)D(P^{-1}P) \\\\cdots (P^{-1}P)DP^{-1} = PD^nP^{-1}\\n\\\\end{aligned}\\n$$\\n\\nY como D es una matriz diagonal\\n\\n$$\\nD^n =\\n\\\\begin{bmatrix}\\n\\\\lambda_1 & 0\\\\\\\\\\n0 & \\\\lambda_2\\n\\\\end{bmatrix}^n\\n=\\n\\\\begin{bmatrix}\\n\\\\lambda_1^n & 0\\\\\\\\\\n0 & \\\\lambda_2^n\\n\\\\end{bmatrix}\\n$$\\n\\npor lo que solo tendríamos que elevar a la $n$ los elementos en la diagonal, calcular $P$ y $P^{-1}$ y expandir $PDP^{-1}$.\\nPara obtener $P$ y $D$ a partir de $A$ podemos usar el [teorema de eigen descomposición](https://mathworld.wolfram.com/EigenDecompositionTheorem.html).\\nDonde D estaría formada por los [eigenvalores](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) de A en\\nlos elementos de su diagonal, es decir $\\\\lambda_i$ para valores de $i \\\\in \\\\{1,2\\\\}$ serían dichos eigenvalores y $P$ estaría definida como una\\n[matriz por bloques](https://en.wikipedia.org/wiki/Block_matrix) de los dos eigenvectores correspondientes a dichos eigenvalores.\\n\\n$$\\nP =\\n\\\\begin{bmatrix}\\n\\\\bm{v_1} & \\\\bm{v_2}\\n\\\\end{bmatrix}\\n$$\\n\\nDonde\\n$$\\n\\\\bm{v1} =\\n\\\\begin{bmatrix}\\nv_{11} \\\\\\\\\\nv_{12}\\n\\\\end{bmatrix} \\\\\\\\\\n\\\\bm{v2} =\\n\\\\begin{bmatrix}\\nv_{21} \\\\\\\\\\nv_{22}\\n\\\\end{bmatrix} \\\\\\\\\\n$$\\n\\nY por lo tanto\\n\\n$$\\nP = \\\\begin{bmatrix}\\nv_{11} & v_{21} \\\\\\\\\\nv_{12} & v_{22} \\\\\\\\\\n\\\\end{bmatrix}\\n$$\\n\\nDonde $\\\\bm{v_1}$ y $\\\\bm{v_2}$ serían esos eigenvectores correspondientes a los eigenvalores $\\\\lambda_1$ y $\\\\lambda_2$\\nrespectivamente.\\n\\nLa diagonalización es posible gracias a que sabemos que los eigenvalores y eigenvectores de A cumplen por definición\\nque\\n\\n$$\\nA\\\\bm{v_i}=\\\\lambda_i\\\\bm{v_i} \\\\quad \\\\forall i \\\\in \\\\{1,2\\\\}\\n$$\\n\\nDe donde podemos ver que\\n\\n$$\\n\\\\begin{aligned}\\nAP &=\\nA\\\\begin{bmatrix}\\n\\\\bm{v_1} & \\\\bm{v_2}\\n\\\\end{bmatrix} \\\\\\\\\\n&= \\\\begin{bmatrix}\\nA \\\\bm{v_1} & A\\\\bm{v_2}\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\begin{bmatrix}\\n\\\\lambda_1 \\\\bm{v_1} & \\\\lambda_2 \\\\bm{v_2}\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\begin{bmatrix}\\n\\\\lambda_1 v_{11} & \\\\lambda_2 v_{21}\\\\\\\\\\n\\\\lambda_1 v_{12} & \\\\lambda_2 v_{22} \\\\\\\\\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\begin{bmatrix}\\nv_{11} & v_{21} \\\\\\\\\\nv_{12} & v_{22} \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n\\\\lambda_1 & 0 \\\\\\\\\\n0 & \\\\lambda_2\\n\\\\end{bmatrix} \\\\\\\\\\n&=PD\\n\\\\end{aligned}\\n$$\\n\\nY multiplicando $P^{-1}$ por la derecha ambos lados de la ecuación obtenemos\\n\\n$$\\n\\\\begin{aligned}\\nAP(P^{-1}) &= PD(P^{-1}) \\\\\\\\\\nA &= PDP^{-1} \\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\nAhora bien, dado que $A\\\\bm{v_i}=\\\\lambda_i\\\\bm{v_i} \\\\quad \\\\forall i \\\\in \\\\{1,2\\\\}$, tenemos que\\n\\n$$\\n\\\\begin{aligned}\\nA\\\\bm{v_i}-\\\\lambda_i\\\\bm{v_i} &= \\\\bm{0} \\\\quad \\\\forall i \\\\in \\\\{1,2\\\\}\\n\\\\end{aligned}\\n$$\\n\\ndonde $\\\\bm{0}$ es el vector cero y factorizando $\\\\bm{v_i}$\\n\\n$$\\n(A - \\\\lambda_i I) \\\\bm{v_i} = \\\\bm{0} \\\\quad \\\\forall i \\\\in \\\\{1,2\\\\}\\n$$\\n\\ndonde $I$ es la matriz identidad. Vemos que solo puede cumplirse esta identidad ya sea cuando todo $\\\\bm{v_i}$ sea igual\\na $\\\\bm{0}$ o bien cuando $\\\\det(A - \\\\lambda_i I) = 0$. Los vectores cero no nos serían útiles para diagonalizar, además de que\\nningún eigenvector puede ser el vector 0, por lo que usaremos la segunda identidad para encontrar los valores de $\\\\lambda_i$\\ny $\\\\bm{v_i}$.\\n\\nPor lo tanto para encontrar los valores de $\\\\lambda_i$ vemos que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\det(A-\\\\lambda I) &=\\n\\\\det \\\\left(\\n\\\\begin{bmatrix}\\n1 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n-\\n\\\\begin{bmatrix}\\n\\\\lambda & 0 \\\\\\\\\\n0 & \\\\lambda\\n\\\\end{bmatrix}\\n\\\\right)\\n&= 0 \\\\\\\\\\n&= \\\\det \\\\left(\\n\\\\begin{bmatrix}\\n1 - \\\\lambda & 1 \\\\\\\\\\n1 & -\\\\lambda\\n\\\\end{bmatrix}\\n\\\\right)\\n&= 0 \\\\\\\\\\n&= \\\\lambda^2 -\\\\lambda -1 &= 0\\n\\\\end{aligned}\\n$$\\n\\n$\\\\lambda^2 -\\\\lambda -1$ es el denominado [polinomio característico](https://en.wikipedia.org/wiki/Characteristic_polynomial).\\nResolviendo para $\\\\lambda$ encontrando las raíces del polinomio tenemos que\\n\\n$$\\n\\\\lambda_1 = \\\\frac{1+\\\\sqrt{5}}{2} = \\\\varphi \\\\\\\\\\n\\\\lambda_2 = \\\\frac{1-\\\\sqrt{5}}{2} = \\\\psi\\n$$\\n\\nAhora tenemos que encontrar los eigenvectores correspondientes para cada $\\\\lambda_i$.\\n\\nEmpezando por $\\\\lambda_1$ (que recordemos $\\\\lambda_1=\\\\varphi$) tenemos que\\n\\n$$\\n\\\\begin{aligned}\\n(A - \\\\lambda_1 I) \\\\bm{v_1} &= \\\\bm{0} \\\\\\\\\\n\\\\begin{bmatrix}\\n1 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n-\\n\\\\begin{bmatrix}\\n\\\\varphi & 0 \\\\\\\\\\n0 & \\\\varphi\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nv_{11} \\\\\\\\\\nv_{12}\\n\\\\end{bmatrix}\\n&= \\\\begin{bmatrix}\\n0 \\\\\\\\\\n0\\n\\\\end{bmatrix} \\\\\\\\\\n\\\\begin{bmatrix}\\n1-\\\\varphi & 1 \\\\\\\\\\n1 & -\\\\varphi\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nv_{11} \\\\\\\\\\nv_{12}\\n\\\\end{bmatrix}\\n&= \\\\begin{bmatrix}\\n0 \\\\\\\\\\n0\\n\\\\end{bmatrix}\\n\\\\end{aligned}\\n$$\\n\\nY de las propiedades anteriores, sabemos que $1-\\\\varphi = \\\\psi$, así como $\\\\psi^{-1}=-\\\\varphi$ por lo que\\n\\n$$\\n\\\\begin{bmatrix}\\n\\\\psi & 1 \\\\\\\\\\n1 & \\\\psi^{-1}\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nv_{11} \\\\\\\\\\nv_{12}\\n\\\\end{bmatrix}\\n=\\n\\\\begin{bmatrix}\\n0 \\\\\\\\\\n0\\n\\\\end{bmatrix}\\n$$\\n\\nReduciendo el sistema de ecuaciones resultante con Gauss-Jordan\\n\\n$$\\n\\\\left(\\\\hspace{-5pt}\\\\begin{array}{cc|c}\\n\\\\psi & 1 & 0 \\\\\\\\\\n1 & \\\\psi^{-1} & 0\\n\\\\end{array}\\\\hspace{-5pt}\\\\right)\\n\\\\xrightarrow{\\\\psi R_2 \\\\to R_2}\\n\\\\left(\\\\hspace{-5pt}\\\\begin{array}{cc|c}\\n\\\\psi & 1 & 0 \\\\\\\\\\n\\\\psi & 1 & 0\\n\\\\end{array}\\\\hspace{-5pt}\\\\right)\\n\\\\xrightarrow{R_1-R_2 \\\\to R_2}\\n\\\\left(\\\\hspace{-5pt}\\\\begin{array}{cc|c}\\n\\\\psi & 1 & 0 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{array}\\\\hspace{-5pt}\\\\right)\\n$$\\n\\nDe donde vemos que\\n\\n$$\\n\\\\psi v_{11}+v_{12}=0\\n$$\\n\\ny podemos tomar $v_{12}=1$ por conveniencia de tal manera que\\n\\n$$\\nv_{11}=\\\\frac{-1}{\\\\psi}=-\\\\psi^{-1} = \\\\varphi\\n$$\\n\\npor lo que el eigenvector $\\\\bm{v_1}$ tomaría la forma\\n\\n$$\\n\\\\bm{v_1}=\\n\\\\begin{bmatrix}\\n\\\\varphi \\\\\\\\\\n1\\n\\\\end{bmatrix}\\n$$\\n\\nPara el caso del eigenvalor $\\\\lambda_2$ (recordando que $\\\\lambda_2=\\\\psi$) tendríamos que\\n$$\\n\\\\begin{aligned}\\n(A - \\\\lambda_2 I) \\\\bm{v_2} &= \\\\bm{0} \\\\\\\\\\n\\\\begin{bmatrix}\\n1 & 1 \\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}\\n-\\n\\\\begin{bmatrix}\\n\\\\psi & 0 \\\\\\\\\\n0 & \\\\psi\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nv_{21} \\\\\\\\\\nv_{22}\\n\\\\end{bmatrix}\\n&= \\\\begin{bmatrix}\\n0 \\\\\\\\\\n0\\n\\\\end{bmatrix} \\\\\\\\\\n\\\\begin{bmatrix}\\n1-\\\\psi & 1 \\\\\\\\\\n1 & -\\\\psi\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nv_{21} \\\\\\\\\\nv_{22}\\n\\\\end{bmatrix}\\n&= \\\\begin{bmatrix}\\n0 \\\\\\\\\\n0\\n\\\\end{bmatrix}\\n\\\\end{aligned}\\n$$\\n\\nY por la simetría de la situación es fácil concluir que\\n\\n$$\\n\\\\bm{v_2}=\\n\\\\begin{bmatrix}\\n\\\\psi \\\\\\\\\\n1\\n\\\\end{bmatrix}\\n$$\\n\\nDe donde\\n\\n$$\\nP=\\n\\\\begin{bmatrix}\\n\\\\varphi & \\\\psi \\\\\\\\\\n1 & 1\\n\\\\end{bmatrix}\\n$$\\n\\ny por lo tanto\\n\\n$$\\nP^{-1} = \\\\frac{1}{\\\\det(P)}\\\\text{adj}(P)\\n$$\\n\\ny dado que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\det(P) &= \\\\varphi - \\\\psi \\\\\\\\\\n&= \\\\frac{1+\\\\sqrt{5}}{2} - \\\\frac{1-\\\\sqrt{5}}{2} \\\\\\\\\\n&= \\\\frac{1+\\\\sqrt{5}-1+\\\\sqrt{5}}{2} \\\\\\\\\\n&= \\\\frac{2\\\\sqrt{5}}{2} \\\\\\\\\\n&= \\\\sqrt{5}\\n\\\\end{aligned}\\n$$\\n\\ny que\\n\\n$$\\n\\\\text{adj}(P) =\\n\\\\begin{bmatrix}\\n1 & - \\\\psi \\\\\\\\\\n-1 & \\\\varphi\\n\\\\end{bmatrix}\\n$$\\n\\nentonces\\n\\n$$\\nP^{-1} = \\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n1 & - \\\\psi \\\\\\\\\\n-1 & \\\\varphi\\n\\\\end{bmatrix}\\n$$\\n\\npor lo que entonces dado que $A^n = PD^nP^{-1}$ tenemos que\\n$$\\n\\\\begin{aligned}\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}^n\\n&=\\n\\\\begin{bmatrix}\\n\\\\varphi & \\\\psi \\\\\\\\\\n1 & 1\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n\\\\varphi & 0\\\\\\\\\\n0 & \\\\psi\\n\\\\end{bmatrix}^n\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n1 & - \\\\psi \\\\\\\\\\n-1 & \\\\varphi\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi & \\\\psi \\\\\\\\\\n1 & 1\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n\\\\varphi^n & 0\\\\\\\\\\n0 & \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 & - \\\\psi \\\\\\\\\\n-1 & \\\\varphi\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi^{n+1} & \\\\psi^{n+1} \\\\\\\\\\n\\\\varphi^n & \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 & - \\\\psi \\\\\\\\\\n-1 & \\\\varphi\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi^{n+1} - \\\\psi^{n+1} & \\\\psi \\\\varphi^{n+1} - \\\\varphi \\\\psi^{n+1}\\\\\\\\\\n\\\\varphi^n - \\\\psi^n & \\\\psi \\\\varphi^n - \\\\varphi \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\end{aligned}\\n$$\\n\\nFinalmente vemos que\\n$$\\n\\\\begin{aligned}\\n\\\\begin{bmatrix}\\nF_{n+1} \\\\\\\\\\nF_{n}\\n\\\\end{bmatrix}\\n&=\\n\\\\begin{bmatrix}\\n1 & 1\\\\\\\\\\n1 & 0\\n\\\\end{bmatrix}^n\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi^{n+1} - \\\\psi^{n+1} & \\\\psi \\\\varphi^{n+1} - \\\\varphi \\\\psi^{n+1}\\\\\\\\\\n\\\\varphi^n - \\\\psi^n & \\\\psi \\\\varphi^n - \\\\varphi \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\nF_1 \\\\\\\\\\nF_0\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi^{n+1} - \\\\psi^{n+1} & \\\\psi \\\\varphi^{n+1} - \\\\varphi \\\\psi^{n+1}\\\\\\\\\\n\\\\varphi^n - \\\\psi^n & \\\\psi \\\\varphi^n - \\\\varphi \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 \\\\\\\\\\n0\\n\\\\end{bmatrix} \\\\\\\\\\n&=\\n\\\\frac{1}{\\\\sqrt{5}}\\n\\\\begin{bmatrix}\\n\\\\varphi^{n+1} - \\\\psi^{n+1} \\\\\\\\\\n\\\\varphi^n - \\\\psi^n\\n\\\\end{bmatrix}\\n\\\\end{aligned}\\n$$\\n\\nDe donde claramente tenemos que\\n\\n$$ F_{n+1} = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^{n+1} - \\\\psi^{n+1} \\\\right] $$\\n\\ny\\n\\n$$ \\\\boxed{ F_n = \\\\frac{1}{\\\\sqrt{5}} \\\\left[ \\\\varphi^n - \\\\psi^n \\\\right] } $$\\n\\nQue es precisamente la fórmula de Binet $\\\\square$.\\n\\n## Referencias\\n\\n- [Wikipedia: Fibonacci Number](https://en.wikipedia.org/wiki/Fibonacci_number)\\n- [Wikipedia: Golden Ratio](https://en.wikipedia.org/wiki/Golden_ratio)\\n- [Wikipedia: Characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial)\\n- [Wolfram: Eigen Decomposition](https://mathworld.wolfram.com/EigenDecomposition.html)\\n- [Wolfram: Eigen Decomposition Theorem](https://mathworld.wolfram.com/EigenDecompositionTheorem.html)\\n- [Hyper-Textbook: Optimization Models and Applications, L. El Ghaoui, EECS Department, UC Berkeley - Spectral Theorem section](https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/l_sym_sed.html)\\n- Fibonacci and Lucas Numbers with Applications, Volume 1, 2nd Edition - Thomas Koshy\\n"}},{"node":{"frontmatter":{"date":"2021-01-09T23:47:46.182Z","description":"Derivación de las fórmulas de regresión lineal simple","title":"Regresión Lineal Simple"},"slug":"linear-reg","rawBody":"---\\ntitle: Regresión Lineal Simple \\ndate: \\"2021-01-09T23:47:46.182Z\\"\\ndescription: \\"Derivación de las fórmulas de regresión lineal simple\\"\\n---\\nLa regresión lineal simple se suele enseñar en la academia en cursos de introducción a la estadística.\\nNormalmente se presentan métodos para calcular los coeficientes de la regresión, pero pocas veces se presenta\\nel origen de dichas fórmulas.\\n\\nEn esta publicación se mostrará la derivación de estas fórmulas para el caso más básico, usando la aproximación\\npor mínimos cuadrados ordinarios (abreviado OLS por las siglas en inglés *Ordinary Least Squares*). \\nLa aproximación por mínimos cuadrados fué publicada por Legendre en 1805 y Gauss en 1809 para determinar, dadas\\nciertas observaciones astronómicas, las órbitas de cuerpos celestes respecto al sol.\\n\\n## Definición del problema\\n\\nDado un conjunto de $n$ pares de datos $(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_n,y_n)$ que se presupone se ajustan a\\nun modelo de regresión\\n$$\\ny =  E(y) + \\\\epsilon\\n$$\\nDonde $y$ es una variable dependiente, $E(y)$ la esperanza matemática de esa variable y $\\\\epsilon$ una\\nvariable aleatoria que representa el error. Si asumimos que se sigue una relación lineal en estos datos y por lo tanto\\n$E(y)$ se aproxima a una función lineal de la forma $E(y) = \\\\beta_0 + \\\\beta_1 x$. La regresión lineal consiste en\\nencontrar los coeficientes $\\\\beta_0$ y $\\\\beta_1$ tales que se ajusten mejor al modelo minimizando el error.\\n\\nTenemos entonces que\\n$$\\ny_i = \\\\beta_0 + \\\\beta_1 x_i + \\\\epsilon_i \\\\text{ para } i = 1,2,...,n\\n$$\\nDonde $\\\\beta_0$ es un coeficiente llamado *\\"intercepto\\"*, $\\\\beta_1$ otro coeficiente llamado *\\"pendiente\\"* de la\\nlinea de regresión, $y_i$ los valores que toma la variable dependiente, $x_i$ los valores que toma la variable\\nindependiente y $\\\\epsilon_i$ los errores respectivos.\\n\\nSe asume que $E(\\\\epsilon) = 0$ con una varianza constante $\\\\text{Var}(\\\\epsilon)=\\\\sigma^2$.\\n## Fórmulas para hayar los coeficientes\\nEn la literatura se pueden encontrar las siguientes fórmulas para obtener los coeficientes aplicando \\nminimos cuadrados ordinarios\\n\\n$$\\n\\\\boxed{\\n\\\\begin{aligned}\\n\\\\beta_1 &= \\\\frac{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})}{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y}-\\\\beta_1\\\\bar{x}\\n\\\\end{aligned}}\\n$$\\n\\ndonde $\\\\bar{x}$ y $\\\\bar{y}$, representan los promedios de $x_i$ y $y_i$ respectivamente, i.e.\\n\\n$$\\n\\\\bar{x} = \\\\frac{\\\\sum_{i=1}^{n}x_i}{n},\\\\text{ }\\n\\\\bar{y} = \\\\frac{\\\\sum_{i=1}^{n}y_i}{n} \\n$$\\n## Derivación de las fórmulas\\nExisten diferentes maneras de llegar a este resultado. En este caso usaremos el cálculo y en otra publicación se\\npresentará una generalización usando el álgebra lineal.\\n\\nComo estamos asumiendo que los datos se ajustan a un modelo de regresión tenemos que el error está dado por\\n\\n$$\\n\\\\epsilon = y - E(y)\\n$$\\n\\npor simplicidad denotaremos a $\\\\widehat{y} = E(y)$ por lo que $\\\\epsilon = y-\\\\widehat{y}$, es decir\\n$$\\n\\\\epsilon_i = y_i - \\\\widehat{y}_i = y_i - (\\\\beta_0 + \\\\beta_i x_i), \\\\forall i \\\\in \\\\{1, ..., n\\\\}\\n$$\\n\\nPodríamos entonces tratar de hayar los valores de $\\\\beta_0$ y $\\\\beta_1$ que minimizen la suma de estos errores. Sin\\nembargo como el valor esperado de los errores es 0, i.e. $E(\\\\epsilon_i)=0$ y $\\\\text{Var}(\\\\epsilon)=\\\\sigma^2$, para\\ntodo $i \\\\in \\\\{1,...,n\\\\}$, los errores podrían tomar valores tanto negativos como postivos, por lo que\\nsimplemente sumar no funcionaría. Podríamos en su lugar optar por minimizar la suma de los valores absolutos de estos\\nerrores en su lugar, sin embargo, aún tendríamos problemas, ya que su manipulación algebráica se complicaría. Por\\nesta razón se opta mas bien por minimizar la suma de los cuadrados del error, a esto se le conoce como\\n*minimos cuadrados ordinarios* (en inglés *Ordinary Least Squeares* abreviado OLS) y se describe así\\n\\n$$\\n\\\\text{Encontrar }\\\\underset{\\\\beta_0,\\\\beta_1}{\\\\min} Q(\\\\beta_0,\\\\beta1), \\\\text{ para }\\n\\\\underset{\\\\beta_0,\\\\beta_1}{\\\\min} Q(\\\\beta_0,\\\\beta1)\\n= \\\\sum_{i=1}^{n}\\\\epsilon_i^2 = \\\\sum_{i=1}^{n}(y_i-\\\\widehat{y}_i)^2\\n$$\\n\\nPodemos entonces usar el [teorema de fermat para puntos estacionarios](https://en.wikipedia.org/w/index.php?title=Fermat\'s_theorem_(stationary_points)&oldid=980603861)\\nrespecto de $\\\\beta_0$ y $\\\\beta_1$. Dicho teorema nos dice que si una funcion tiene un *extremum* local o punto crítico,\\nentonces ese punto se haya cuando la derivada de esa función es 0. Aunque no se demuestra en esta publicación, sabemos\\nque siempre existe solo un extremum y este es mínimo o *minimum* global para este problema, ya que este es un problema\\nde [optimización convexa](https://en.wikipedia.org/w/index.php?title=Convex_optimization&oldid=1000262013).\\nPor el momento se tendrá que asumir este hecho como verdadero, en una siguiente publicación compartiré la demostración.\\n\\nDado que $ \\\\sum_{i=1}^{n}(y_i-\\\\widehat{y}_i)^2 = \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x)^2 $ usando el teorema de\\nfermat para puntos estacionarios para $\\\\beta_0$ y $\\\\beta_1$ respectivamente obtendremos los mínimos resolviendo el\\nsistema de ecuaciones\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_0} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 &= 0 \\\\\\\\\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_1} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 &= 0\\n\\\\end{aligned}\\n$$\\n\\nDado que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_0} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2\\n&= \\\\sum_{i=1}^{n} \\\\frac{\\\\partial}{\\\\partial \\\\beta_0}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 \\\\\\\\\\n&= \\\\sum_{i=1}^{n} 2(y_i-\\\\beta_o - \\\\beta_1 x_i)(-1) \\\\\\\\\\n&= -2 \\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) \\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\ny que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_1} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2\\n&= \\\\sum_{i=1}^{n} \\\\frac{\\\\partial}{\\\\partial \\\\beta_1}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 \\\\\\\\\\n&= \\\\sum_{i=1}^{n} 2(y_i-\\\\beta_o - \\\\beta_1 x_i)(-x_i) \\\\\\\\\\n&= -2 \\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i)\\n\\\\end{aligned}\\n$$\\n\\nEl problema se reduce a resolver el sistema de ecuaciones\\n\\n$$\\n\\\\begin{aligned}\\n-2 \\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x) &= 0 \\\\\\\\\\n-2 \\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0\\n\\\\end{aligned}\\n$$\\n\\nNote que si en ambas ecuaciones dividimos ambas partes por $-2$ obtendríamos el sistema\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0\\n\\\\end{aligned}\\n$$\\n\\nDe la primera ecuación obtenemos\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - \\\\sum_{i=1}^{n} \\\\beta_o - \\\\sum_{i=1}^{n} \\\\beta_1 x_i &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - \\\\beta_o \\\\sum_{i=1}^{n}(1)  - \\\\beta_1 \\\\sum_{i=1}^{n} x_i &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - n \\\\beta_o  - \\\\beta_1 \\\\sum_{i=1}^{n} x_i &= 0 \\\\\\\\\\n\\\\end{aligned}\\n$$\\ny resolviendo para $\\\\beta_0$ obtenemos\\n\\n$$\\n\\\\begin{aligned}\\n\\\\beta_0 &= \\\\frac{\\\\sum_{i=1}^{n} y_i - \\\\beta_1 \\\\sum_{i=1}^{n} x_i}{n} \\\\\\\\\\n\\\\beta_0 &= \\\\frac{\\\\sum_{i=1}^{n} y_i}{n}- \\\\beta_1 \\\\frac{ \\\\sum_{i=1}^{n} x_i}{n} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y} - \\\\beta_1 \\\\bar{x}\\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\nQue es justamente la primera fórmula a la que queríamos llegar, hace falta la fórmula para $\\\\beta_1$. De la\\nsegunda ecuación del sistema de ecuaciones sustituyendo $\\\\beta_0$ por $\\\\bar{y} - \\\\beta_1 \\\\bar{x}$ obtenemos\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i-(\\\\bar{y} - \\\\beta_1 \\\\bar{x}) - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i- \\\\bar{y} + \\\\beta_1 \\\\bar{x} - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y} - \\\\beta_1 (x_i - \\\\bar{x})) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y}) - \\\\sum_{i=1}^{n} \\\\beta_1 x_i (x_i - \\\\bar{x}) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y}) - \\\\beta_1 \\\\sum_{i=1}^{n} x_i (x_i - \\\\bar{x}) &= 0\\n\\\\end{aligned}\\n$$\\ny resolviendo para $\\\\beta_1$ obtenemos que\\n$$\\n\\\\boxed{\\\\beta_1 = \\\\frac{\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y})}{\\\\sum_{i=1}^{n} x_i (x_i - \\\\bar{x})}}\\n$$\\nEsta en efécto es una fórmula equivalente. Para demostrar esto vemos que solo hace falta comprobar que el numerador\\nes equivalente a $\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})$ y el denominador a $\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2$\\nrespectivamente.\\n\\nPara el primer caso tenemos que\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y}) &=\\\\sum_{i=1}^{n} (x_i(y_i-\\\\bar{y})-\\\\bar{x}(y_i-\\\\bar{y})) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\sum_{i=1}^{n} \\\\bar{x}(y_i-\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} \\\\sum_{i=1}^{n} (y_i-\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\sum_{i=1}^{n}\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\bar{y}\\\\sum_{i=1}^{n}(1)) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - n \\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - n \\\\frac{\\\\sum_{i=1}^{n}y_i}{n}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\sum_{i=1}^{n}y_i) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (0) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y})\\n\\\\end{aligned}\\n$$\\nDe igual manera tenemos que\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2 &= \\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(x_i-\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}(x_i(x_i-\\\\bar{x})-\\\\bar{x}(x_i-\\\\bar{x})) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\sum_{i=1}^{n}\\\\bar{x}(x_i - \\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}\\\\sum_{i=1}^{n}(x_i - \\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\sum_{i=1}^{n}\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\bar{x}\\\\sum_{i=1}^{n}(1)) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - n\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - n \\\\frac{\\\\sum_{i=1}^{n}x_i}{n}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\sum_{i=1}^{n}x_i) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(0) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})\\n\\\\end{aligned}\\n$$\\nDe donde claramente tenemos que\\n$$\\n\\\\boxed{\\n\\\\begin{aligned}\\n\\\\beta_1 &= \\\\frac{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})}{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y}-\\\\beta_1\\\\bar{x}\\n\\\\end{aligned}}\\n$$\\nque justamente son las fórmulas que queríamos encontrar. $\\\\square$\\n\\n## Referencias\\n- [Derivation of simple linear regression formula - Ivanky site](https://ivanky.wordpress.com/2018/05/20/derivation-of-simple-linear-regression-formula/)\\n- Linear Regression Analysis - Theory and Computing. Xin Yan, Xiao Gang Su\\n"}}]}}}')},UkwQ:function(n,e,a){"use strict";a.d(e,"a",(function(){return d}));var i=a("Mtj7"),r=a("q1tI"),o=a.n(r),t=a("Wbzz"),s=a("CHYI"),l=a("wqpA");a("jL2j");function d(n){n.data;var e=m();return e?o.a.createElement(o.a.Fragment,null,o.a.createElement("section",{className:"Posts"},o.a.createElement("h2",{className:"Posts__banner"},"Posts"),e.edges.map((function(n,e){return o.a.createElement("div",{className:"Post",key:e},o.a.createElement(t.Link,{to:"/posts/"+n.node.slug,className:"Post__metainfo"},o.a.createElement("h3",{className:"Post__title"},n.node.frontmatter.title),o.a.createElement("h5",{className:"Post__date"},Object(s.a)(n.node.frontmatter.date))),o.a.createElement("div",{className:"Post__description"},n.node.frontmatter.description.length>1?Object(l.a)(n.node.frontmatter.description,265):Object(l.a)(n.node.rawBody,265)),o.a.createElement(t.Link,{to:"/posts/"+n.node.slug,className:"Post__readmore"},"Read more"))})))):o.a.createElement("h2",{style:{textAlign:"center",marginTop:"50px",color:"gray"}},"No posts found.")}var m=function(){return i.data.allMdx}},jL2j:function(n,e,a){},wqpA:function(n,e,a){"use strict";function i(n,e){return n.length<=e?n:n.slice(0,e)+"..."}a.d(e,"a",(function(){return i}))}}]);
//# sourceMappingURL=7a2d2f0d00fa1c65e125eaadfd8361e2893b322a-f80d61af2fd0350fe1db.js.map