(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{CHYI:function(e,a,n){"use strict";n.d(a,"a",(function(){return s}));n("HQhv");var i=["Enero","Febrero","Marzo","April","Mayo","Junio","Julio","Agosto","Septiembre","Octubre","Noviembre","Diciembre"];function s(e){var a;return a=e.split("-"),parseInt(a[2])+" de "+i[parseInt(a[1])-1]+", "+a[0]}},EKbp:function(e){e.exports=JSON.parse('{"data":{"site":{"siteMetadata":{"author":"Miguel Angel Carrasco","description":"Un blog con una compilación de notas en matemáticas, ciencias de la computación, programación y otros temas interesantes, usado principalmente para recordar y compartir ideas.","title":"Ideas Interesantes"}}}}')},Mtj7:function(e){e.exports=JSON.parse('{"data":{"allMdx":{"edges":[{"node":{"frontmatter":{"date":"2021-01-09T23:47:46.182Z","description":"Derivación de las fórmulas de regresión lineal simple","title":"Regresión Lineal Simple"},"slug":"linear-reg","rawBody":"---\\ntitle: Regresión Lineal Simple \\ndate: \\"2021-01-09T23:47:46.182Z\\"\\ndescription: \\"Derivación de las fórmulas de regresión lineal simple\\"\\n---\\nLa regresión lineal simple se suele enseñar en la academia en cursos de introducción a la estadística.\\nNormalmente se presentan métodos para calcular los coeficientes de la regresión, pero pocas veces se presenta\\nel origen de dichas fórmulas.\\n\\nEn esta publicación se mostrará la derivación de estas fórmulas para el caso mas básico, usando la aproximación \\npor mínimos cuadrados ordinarios (abreviado OLS por las siglas en inglés *Ordinary Least Squares*). \\nLa aproximación por mínimos cuadrados fué publicada por Legendre en 1805 y Gauss en 1809 para determinar, dadas\\nciertas observaciones astronómicas, las órbitas de cuerpos celestes respecto al sol.\\n\\n## Definición del problema\\n\\nDado un conjunto de $n$ pares de datos $(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_n,y_n)$ que se presupone se ajustan a\\nun modelo de regresión\\n$$\\ny =  E(y) + \\\\epsilon\\n$$\\nDonde $y$ es una variable dependiente, $E(y)$ la esperanza matemática de esa variable y $\\\\epsilon$ una\\nvariable aleatoria que representa el error. Si asumimos que se sigue una relación lineal en estos datos y por lo tanto\\n$E(y)$ se aproxima a una función lineal de la forma $E(y) = \\\\beta_0 + \\\\beta_1 x$. La regresión lineal consiste en\\nencontrar los coeficientes $\\\\beta_0$ y $\\\\beta_1$ tales que se ajusten mejor al modelo minimizando el error.\\n\\nTenemos entonces que\\n$$\\ny_i = \\\\beta_0 + \\\\beta_1 x_i + \\\\epsilon_i \\\\text{ para } i = 1,2,...,n\\n$$\\nDonde $\\\\beta_0$ es un coeficiente llamado *\\"intercepto\\"*, $\\\\beta_1$ otro coeficiente llamado *\\"pendiente\\"* de la\\nlinea de regresión, $y_i$ los valores que toma la variable dependiente, $x_i$ los valores que toma la variable\\nindependiente y $\\\\epsilon_i$ los errores respectivos.\\n\\nSe asume que $E(\\\\epsilon) = 0$ con una varianza constante $\\\\text{Var}(\\\\epsilon)=\\\\sigma^2$.\\n## Fórmulas para hayar los coeficientes\\nEn la literatura se pueden encontrar las siguientes fórmulas para obtener los coeficientes aplicando \\nminimos cuadrados ordinarios\\n\\n$$\\n\\\\boxed{\\n\\\\begin{aligned}\\n\\\\beta_1 &= \\\\frac{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})}{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y}-\\\\beta_1\\\\bar{x}\\n\\\\end{aligned}}\\n$$\\n\\ndonde $\\\\bar{x}$ y $\\\\bar{y}$, representan los promedios de $x_i$ y $y_i$ respectivamente, i.e.\\n\\n$$\\n\\\\bar{x} = \\\\frac{\\\\sum_{i=1}^{n}x_i}{n},\\\\text{ }\\n\\\\bar{y} = \\\\frac{\\\\sum_{i=1}^{n}y_i}{n} \\n$$\\n## Derivación de las fórmulas\\nExisten diferentes maneras de llegar a este resultado. En este caso usaremos el cálculo y en otra publicación se\\npresentará una generalización usando el álgebra lineal.\\n\\nComo estamos asumiendo que los datos se ajustan a un modelo de regresión tenemos que el error está dado por\\n\\n$$\\n\\\\epsilon = y - E(y)\\n$$\\n\\npor simplicidad denotaremos a $\\\\widehat{y} = E(y)$ por lo que $\\\\epsilon = y-\\\\widehat{y}$, es decir\\n$$\\n\\\\epsilon_i = y_i - \\\\widehat{y}_i = y_i - (\\\\beta_0 + \\\\beta_i x_i), \\\\forall i \\\\in \\\\{1, ..., n\\\\}\\n$$\\n\\nPodríamos entonces tratar de hayar los valores de $\\\\beta_0$ y $\\\\beta_1$ que minimizen la suma de estos errores. Sin\\nembargo como el valor esperado de los errores es 0, i.e. $E(\\\\epsilon_i)=0$ y $\\\\text{Var}(\\\\epsilon)=\\\\sigma^2$, para\\ntodo $i \\\\in \\\\{1,...,n\\\\}$, los errores podrían tomar valores tanto negativos como postivos, por lo que\\nsimplemente sumar no funcionaría. Podríamos en su lugar optar por minimizar la suma de los valores absolutos de estos\\nerrores en su lugar, sin embargo, aún tendríamos problemas, ya que su manipulación algebráica se complicaría. Por\\nesta razón se opta mas bien por minimizar la suma de los cuadrados del error, a esto se le conoce como\\n*minimos cuadrados ordinarios* (en inglés *Ordinary Least Squeares* abreviado OLS) y se describe así\\n\\n$$\\n\\\\text{Encontrar }\\\\underset{\\\\beta_0,\\\\beta_1}{\\\\min} Q(\\\\beta_0,\\\\beta1), \\\\text{ para }\\n\\\\underset{\\\\beta_0,\\\\beta_1}{\\\\min} Q(\\\\beta_0,\\\\beta1)\\n= \\\\sum_{i=1}^{n}\\\\epsilon_i^2 = \\\\sum_{i=1}^{n}(y_i-\\\\widehat{y}_i)^2\\n$$\\n\\nPodemos entonces usar el [teorema de fermat para puntos estacionarios](https://en.wikipedia.org/w/index.php?title=Fermat\'s_theorem_(stationary_points)&oldid=980603861)\\nrespecto de $\\\\beta_0$ y $\\\\beta_1$. Dicho teorema nos dice que si una funcion tiene un *extremum* local o punto crítico,\\nentonces ese punto se haya cuando la derivada de esa función es 0. Aunque no se demuestra en esta publicación, sabemos\\nque siempre existe solo un extremum y este es mínimo o *minimum* global para este problema, ya que este es un problema\\nde [optimización convexa](https://en.wikipedia.org/w/index.php?title=Convex_optimization&oldid=1000262013).\\nPor el momento se tendrá que asumir este hecho como verdadero, en una siguiente publicación compartiré la demostración.\\n\\nDado que $ \\\\sum_{i=1}^{n}(y_i-\\\\widehat{y}_i)^2 = \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x)^2 $ usando el teorema de\\nfermat para puntos estacionarios para $\\\\beta_0$ y $\\\\beta_1$ respectivamente obtendremos los mínimos resolviendo el\\nsistema de ecuaciones\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_0} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 &= 0 \\\\\\\\\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_1} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 &= 0\\n\\\\end{aligned}\\n$$\\n\\nDado que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_0} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2\\n&= \\\\sum_{i=1}^{n} \\\\frac{\\\\partial}{\\\\partial \\\\beta_0}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 \\\\\\\\\\n&= \\\\sum_{i=1}^{n} 2(y_i-\\\\beta_o - \\\\beta_1 x_i)(-1) \\\\\\\\\\n&= -2 \\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) \\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\ny que\\n\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\partial}{\\\\partial \\\\beta_1} \\\\sum_{i=1}^{n}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2\\n&= \\\\sum_{i=1}^{n} \\\\frac{\\\\partial}{\\\\partial \\\\beta_1}(y_i - \\\\beta_0 - \\\\beta_1 x_i)^2 \\\\\\\\\\n&= \\\\sum_{i=1}^{n} 2(y_i-\\\\beta_o - \\\\beta_1 x_i)(-x_i) \\\\\\\\\\n&= -2 \\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i)\\n\\\\end{aligned}\\n$$\\n\\nEl problema se reduce a resolver el sistema de ecuaciones\\n\\n$$\\n\\\\begin{aligned}\\n-2 \\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x) &= 0 \\\\\\\\\\n-2 \\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0\\n\\\\end{aligned}\\n$$\\n\\nNote que si en ambas ecuaciones dividimos ambas partes por $-2$ obtendríamos el sistema\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0\\n\\\\end{aligned}\\n$$\\n\\nDe la primera ecuación obtenemos\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} (y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - \\\\sum_{i=1}^{n} \\\\beta_o - \\\\sum_{i=1}^{n} \\\\beta_1 x_i &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - \\\\beta_o \\\\sum_{i=1}^{n}(1)  - \\\\beta_1 \\\\sum_{i=1}^{n} x_i &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} y_i - n \\\\beta_o  - \\\\beta_1 \\\\sum_{i=1}^{n} x_i &= 0 \\\\\\\\\\n\\\\end{aligned}\\n$$\\ny resolviendo para $\\\\beta_0$ obtenemos\\n\\n$$\\n\\\\begin{aligned}\\n\\\\beta_0 &= \\\\frac{\\\\sum_{i=1}^{n} y_i - \\\\beta_1 \\\\sum_{i=1}^{n} x_i}{n} \\\\\\\\\\n\\\\beta_0 &= \\\\frac{\\\\sum_{i=1}^{n} y_i}{n}- \\\\beta_1 \\\\frac{ \\\\sum_{i=1}^{n} x_i}{n} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y} - \\\\beta_1 \\\\bar{x}\\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\nQue es justamente la primera fórmula a la que queríamos llegar, hace falta la fórmula para $\\\\beta_1$. De la\\nsegunda ecuación del sistema de ecuaciones sustituyendo $\\\\beta_0$ por $\\\\bar{y} - \\\\beta_1 \\\\bar{x}$ obtenemos\\n\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n} x_i(y_i-\\\\beta_o - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i-(\\\\bar{y} - \\\\beta_1 \\\\bar{x}) - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i- \\\\bar{y} + \\\\beta_1 \\\\bar{x} - \\\\beta_1 x_i) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y} - \\\\beta_1 (x_i - \\\\bar{x})) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y}) - \\\\sum_{i=1}^{n} \\\\beta_1 x_i (x_i - \\\\bar{x}) &= 0 \\\\\\\\\\n\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y}) - \\\\beta_1 \\\\sum_{i=1}^{n} x_i (x_i - \\\\bar{x}) &= 0\\n\\\\end{aligned}\\n$$\\ny resolviendo para $\\\\beta_1$ obtenemos que\\n$$\\n\\\\boxed{\\\\beta_1 = \\\\frac{\\\\sum_{i=1}^{n} x_i(y_i - \\\\bar{y})}{\\\\sum_{i=1}^{n} x_i (x_i - \\\\bar{x})}}\\n$$\\nEsta en efécto es una fórmula equivalente. Para demostrar esto vemos que solo hace falta comprobar que el numerador\\nes equivalente a $\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})$ y el denominador a $\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2$\\nrespectivamente.\\n\\nPara el primer caso tenemos que\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y}) &=\\\\sum_{i=1}^{n} (x_i(y_i-\\\\bar{y})-\\\\bar{x}(y_i-\\\\bar{y})) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\sum_{i=1}^{n} \\\\bar{x}(y_i-\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} \\\\sum_{i=1}^{n} (y_i-\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\sum_{i=1}^{n}\\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\bar{y}\\\\sum_{i=1}^{n}(1)) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - n \\\\bar{y}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - n \\\\frac{\\\\sum_{i=1}^{n}y_i}{n}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (\\\\sum_{i=1}^{n}y_i - \\\\sum_{i=1}^{n}y_i) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y}) - \\\\bar{x} (0) \\\\\\\\\\n&= \\\\sum_{i=1}^{n} x_i(y_i-\\\\bar{y})\\n\\\\end{aligned}\\n$$\\nDe igual manera tenemos que\\n$$\\n\\\\begin{aligned}\\n\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2 &= \\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(x_i-\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}(x_i(x_i-\\\\bar{x})-\\\\bar{x}(x_i-\\\\bar{x})) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\sum_{i=1}^{n}\\\\bar{x}(x_i - \\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}\\\\sum_{i=1}^{n}(x_i - \\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\sum_{i=1}^{n}\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\bar{x}\\\\sum_{i=1}^{n}(1)) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - n\\\\bar{x}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - n \\\\frac{\\\\sum_{i=1}^{n}x_i}{n}) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(\\\\sum_{i=1}^{n}x_i - \\\\sum_{i=1}^{n}x_i) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})-\\\\bar{x}(0) \\\\\\\\\\n&= \\\\sum_{i=1}^{n}x_i(x_i-\\\\bar{x})\\n\\\\end{aligned}\\n$$\\nDe donde claramente tenemos que\\n$$\\n\\\\boxed{\\n\\\\begin{aligned}\\n\\\\beta_1 &= \\\\frac{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})(y_i-\\\\bar{y})}{\\\\sum_{i=1}^{n}(x_i-\\\\bar{x})^2} \\\\\\\\\\n\\\\beta_0 &= \\\\bar{y}-\\\\beta_1\\\\bar{x}\\n\\\\end{aligned}}\\n$$\\nque justamente son las fórmulas que queríamos encontrar. $\\\\square$\\n\\n## Referencias\\n- [Derivation of simple linear regression formula - Ivanky site](https://ivanky.wordpress.com/2018/05/20/derivation-of-simple-linear-regression-formula/)\\n- Linear Regression Analysis - Theory and Computing. Xin Yan, Xiao Gang Su\\n"}}]}}}')},Q9AM:function(e,a,n){},QeBL:function(e,a,n){"use strict";n.r(a),n.d(a,"default",(function(){return m}));var i=n("EKbp"),s=n("q1tI"),r=n.n(s),t=n("sNzC"),o=n("cElI");n("Q9AM");function _(e){return e?r.a.createElement("section",{className:"Intro"},r.a.createElement("h1",{className:"Intro__header"},e.siteTitle&&""!==e.siteTitle?e.siteTitle:"Undefined"),r.a.createElement("h4",{className:"Intro__subheader"},e.siteDescription&&""!==e.siteDescription?e.siteDescription:"Undefined")):r.a.createElement("div",null,"Props are empty")}var l=n("UkwQ");function m(e){var a=i.data.site;return r.a.createElement(t.a,null,r.a.createElement(o.a,{title:a.siteMetadata.title}),r.a.createElement(_,{siteTitle:a.siteMetadata.title,siteDescription:a.siteMetadata.description,siteAuthor:a.siteMetadata.author}),r.a.createElement(l.a,null))}},UkwQ:function(e,a,n){"use strict";n.d(a,"a",(function(){return l}));var i=n("Mtj7"),s=n("q1tI"),r=n.n(s),t=n("Wbzz"),o=n("CHYI"),_=n("wqpA");n("jL2j");function l(e){e.data;var a=m();return a?r.a.createElement(r.a.Fragment,null,r.a.createElement("section",{className:"Posts"},r.a.createElement("h2",{className:"Posts__banner"},"Posts"),a.edges.map((function(e,a){return r.a.createElement("div",{className:"Post",key:a},r.a.createElement(t.Link,{to:"/posts/"+e.node.slug,className:"Post__metainfo"},r.a.createElement("h3",{className:"Post__title"},e.node.frontmatter.title),r.a.createElement("h5",{className:"Post__date"},Object(o.a)(e.node.frontmatter.date))),r.a.createElement("div",{className:"Post__description"},e.node.frontmatter.description.length>1?Object(_.a)(e.node.frontmatter.description,265):Object(_.a)(e.node.rawBody,265)),r.a.createElement(t.Link,{to:"/posts/"+e.node.slug,className:"Post__readmore"},"Read more"))})))):r.a.createElement("h2",{style:{textAlign:"center",marginTop:"50px",color:"gray"}},"No posts found.")}var m=function(){return i.data.allMdx}},jL2j:function(e,a,n){},wqpA:function(e,a,n){"use strict";function i(e,a){return e.length<=a?e:e.slice(0,a)+"..."}n.d(a,"a",(function(){return i}))}}]);
//# sourceMappingURL=component---src-pages-index-tsx-74cf3969afea62bf4eef.js.map