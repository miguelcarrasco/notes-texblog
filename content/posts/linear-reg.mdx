---
title: Regresión Lineal Simple 
date: "2021-01-09T23:47:46.182Z"
description: "Derivación de las fórmulas de regresión lineal simple"
---
La regresión lineal simple se suele enseñar en la academia en cursos de introducción a la estadística.
Normalmente se presentan métodos para calcular los coeficientes de la regresión, pero pocas veces se presenta
el origen de dichas fórmulas.

En esta publicación se mostrará la derivación de estas fórmulas para el caso más básico, usando la aproximación
por mínimos cuadrados ordinarios (abreviado OLS por las siglas en inglés *Ordinary Least Squares*). 
La aproximación por mínimos cuadrados fué publicada por Legendre en 1805 y Gauss en 1809 para determinar, dadas
ciertas observaciones astronómicas, las órbitas de cuerpos celestes respecto al sol.

## Definición del problema

Dado un conjunto de $n$ pares de datos $(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_n,y_n)$ que se presupone se ajustan a
un modelo de regresión
$$
y =  E(y) + \epsilon
$$
Donde $y$ es una variable dependiente, $E(y)$ la esperanza matemática de esa variable y $\epsilon$ una
variable aleatoria que representa el error. Si asumimos que se sigue una relación lineal en estos datos y por lo tanto
$E(y)$ se aproxima a una función lineal de la forma $E(y) = \beta_0 + \beta_1 x$. La regresión lineal consiste en
encontrar los coeficientes $\beta_0$ y $\beta_1$ tales que se ajusten mejor al modelo minimizando el error.

Tenemos entonces que
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i \text{ para } i = 1,2,...,n
$$
Donde $\beta_0$ es un coeficiente llamado *"intercepto"*, $\beta_1$ otro coeficiente llamado *"pendiente"* de la
linea de regresión, $y_i$ los valores que toma la variable dependiente, $x_i$ los valores que toma la variable
independiente y $\epsilon_i$ los errores respectivos.

Se asume que $E(\epsilon) = 0$ con una varianza constante $\text{Var}(\epsilon)=\sigma^2$.
## Fórmulas para hayar los coeficientes
En la literatura se pueden encontrar las siguientes fórmulas para obtener los coeficientes aplicando 
minimos cuadrados ordinarios

$$
\boxed{
\begin{aligned}
\beta_1 &= \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \\
\beta_0 &= \bar{y}-\beta_1\bar{x}
\end{aligned}}
$$

donde $\bar{x}$ y $\bar{y}$, representan los promedios de $x_i$ y $y_i$ respectivamente, i.e.

$$
\bar{x} = \frac{\sum_{i=1}^{n}x_i}{n},\text{ }
\bar{y} = \frac{\sum_{i=1}^{n}y_i}{n} 
$$
## Derivación de las fórmulas
Existen diferentes maneras de llegar a este resultado. En este caso usaremos el cálculo y en otra publicación se
presentará una generalización usando el álgebra lineal.

Como estamos asumiendo que los datos se ajustan a un modelo de regresión tenemos que el error está dado por

$$
\epsilon = y - E(y)
$$

por simplicidad denotaremos a $\widehat{y} = E(y)$ por lo que $\epsilon = y-\widehat{y}$, es decir
$$
\epsilon_i = y_i - \widehat{y}_i = y_i - (\beta_0 + \beta_i x_i), \forall i \in \{1, ..., n\}
$$

Podríamos entonces tratar de hayar los valores de $\beta_0$ y $\beta_1$ que minimizen la suma de estos errores. Sin
embargo como el valor esperado de los errores es 0, i.e. $E(\epsilon_i)=0$ y $\text{Var}(\epsilon)=\sigma^2$, para
todo $i \in \{1,...,n\}$, los errores podrían tomar valores tanto negativos como postivos, por lo que
simplemente sumar no funcionaría. Podríamos en su lugar optar por minimizar la suma de los valores absolutos de estos
errores en su lugar, sin embargo, aún tendríamos problemas, ya que su manipulación algebráica se complicaría. Por
esta razón se opta mas bien por minimizar la suma de los cuadrados del error, a esto se le conoce como
*minimos cuadrados ordinarios* (en inglés *Ordinary Least Squeares* abreviado OLS) y se describe así

$$
\text{Encontrar }\underset{\beta_0,\beta_1}{\min} Q(\beta_0,\beta1), \text{ para }
\underset{\beta_0,\beta_1}{\min} Q(\beta_0,\beta1)
= \sum_{i=1}^{n}\epsilon_i^2 = \sum_{i=1}^{n}(y_i-\widehat{y}_i)^2
$$

Podemos entonces usar el [teorema de fermat para puntos estacionarios](https://en.wikipedia.org/w/index.php?title=Fermat's_theorem_(stationary_points)&oldid=980603861)
respecto de $\beta_0$ y $\beta_1$. Dicho teorema nos dice que si una funcion tiene un *extremum* local o punto crítico,
entonces ese punto se haya cuando la derivada de esa función es 0. Aunque no se demuestra en esta publicación, sabemos
que siempre existe solo un extremum y este es mínimo o *minimum* global para este problema, ya que este es un problema
de [optimización convexa](https://en.wikipedia.org/w/index.php?title=Convex_optimization&oldid=1000262013).
Por el momento se tendrá que asumir este hecho como verdadero, en una siguiente publicación compartiré la demostración.

Dado que $ \sum_{i=1}^{n}(y_i-\widehat{y}_i)^2 = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x)^2 $ usando el teorema de
fermat para puntos estacionarios para $\beta_0$ y $\beta_1$ respectivamente obtendremos los mínimos resolviendo el
sistema de ecuaciones

$$
\begin{aligned}
\frac{\partial}{\partial \beta_0} \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2 &= 0 \\
\frac{\partial}{\partial \beta_1} \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2 &= 0
\end{aligned}
$$

Dado que

$$
\begin{aligned}
\frac{\partial}{\partial \beta_0} \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
&= \sum_{i=1}^{n} \frac{\partial}{\partial \beta_0}(y_i - \beta_0 - \beta_1 x_i)^2 \\
&= \sum_{i=1}^{n} 2(y_i-\beta_o - \beta_1 x_i)(-1) \\
&= -2 \sum_{i=1}^{n} (y_i-\beta_o - \beta_1 x_i) \\
\end{aligned}
$$

y que

$$
\begin{aligned}
\frac{\partial}{\partial \beta_1} \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
&= \sum_{i=1}^{n} \frac{\partial}{\partial \beta_1}(y_i - \beta_0 - \beta_1 x_i)^2 \\
&= \sum_{i=1}^{n} 2(y_i-\beta_o - \beta_1 x_i)(-x_i) \\
&= -2 \sum_{i=1}^{n} x_i(y_i-\beta_o - \beta_1 x_i)
\end{aligned}
$$

El problema se reduce a resolver el sistema de ecuaciones

$$
\begin{aligned}
-2 \sum_{i=1}^{n} (y_i-\beta_o - \beta_1 x) &= 0 \\
-2 \sum_{i=1}^{n} x_i(y_i-\beta_o - \beta_1 x_i) &= 0
\end{aligned}
$$

Note que si en ambas ecuaciones dividimos ambas partes por $-2$ obtendríamos el sistema

$$
\begin{aligned}
\sum_{i=1}^{n} (y_i-\beta_o - \beta_1 x_i) &= 0 \\
\sum_{i=1}^{n} x_i(y_i-\beta_o - \beta_1 x_i) &= 0
\end{aligned}
$$

De la primera ecuación obtenemos
$$
\begin{aligned}
\sum_{i=1}^{n} (y_i-\beta_o - \beta_1 x_i) &= 0 \\
\sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \beta_o - \sum_{i=1}^{n} \beta_1 x_i &= 0 \\
\sum_{i=1}^{n} y_i - \beta_o \sum_{i=1}^{n}(1)  - \beta_1 \sum_{i=1}^{n} x_i &= 0 \\
\sum_{i=1}^{n} y_i - n \beta_o  - \beta_1 \sum_{i=1}^{n} x_i &= 0 \\
\end{aligned}
$$
y resolviendo para $\beta_0$ obtenemos

$$
\begin{aligned}
\beta_0 &= \frac{\sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i}{n} \\
\beta_0 &= \frac{\sum_{i=1}^{n} y_i}{n}- \beta_1 \frac{ \sum_{i=1}^{n} x_i}{n} \\
\beta_0 &= \bar{y} - \beta_1 \bar{x}\\
\end{aligned}
$$

Que es justamente la primera fórmula a la que queríamos llegar, hace falta la fórmula para $\beta_1$. De la
segunda ecuación del sistema de ecuaciones sustituyendo $\beta_0$ por $\bar{y} - \beta_1 \bar{x}$ obtenemos

$$
\begin{aligned}
\sum_{i=1}^{n} x_i(y_i-\beta_o - \beta_1 x_i) &= 0 \\
\sum_{i=1}^{n} x_i(y_i-(\bar{y} - \beta_1 \bar{x}) - \beta_1 x_i) &= 0 \\
\sum_{i=1}^{n} x_i(y_i- \bar{y} + \beta_1 \bar{x} - \beta_1 x_i) &= 0 \\
\sum_{i=1}^{n} x_i(y_i - \bar{y} - \beta_1 (x_i - \bar{x})) &= 0 \\
\sum_{i=1}^{n} x_i(y_i - \bar{y}) - \sum_{i=1}^{n} \beta_1 x_i (x_i - \bar{x}) &= 0 \\
\sum_{i=1}^{n} x_i(y_i - \bar{y}) - \beta_1 \sum_{i=1}^{n} x_i (x_i - \bar{x}) &= 0
\end{aligned}
$$
y resolviendo para $\beta_1$ obtenemos que
$$
\boxed{\beta_1 = \frac{\sum_{i=1}^{n} x_i(y_i - \bar{y})}{\sum_{i=1}^{n} x_i (x_i - \bar{x})}}
$$
Esta en efécto es una fórmula equivalente. Para demostrar esto vemos que solo hace falta comprobar que el numerador
es equivalente a $\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$ y el denominador a $\sum_{i=1}^{n}(x_i-\bar{x})^2$
respectivamente.

Para el primer caso tenemos que
$$
\begin{aligned}
\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) &=\sum_{i=1}^{n} (x_i(y_i-\bar{y})-\bar{x}(y_i-\bar{y})) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \sum_{i=1}^{n} \bar{x}(y_i-\bar{y}) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} \sum_{i=1}^{n} (y_i-\bar{y}) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (\sum_{i=1}^{n}y_i - \sum_{i=1}^{n}\bar{y}) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (\sum_{i=1}^{n}y_i - \bar{y}\sum_{i=1}^{n}(1)) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (\sum_{i=1}^{n}y_i - n \bar{y}) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (\sum_{i=1}^{n}y_i - n \frac{\sum_{i=1}^{n}y_i}{n}) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (\sum_{i=1}^{n}y_i - \sum_{i=1}^{n}y_i) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y}) - \bar{x} (0) \\
&= \sum_{i=1}^{n} x_i(y_i-\bar{y})
\end{aligned}
$$
De igual manera tenemos que
$$
\begin{aligned}
\sum_{i=1}^{n}(x_i-\bar{x})^2 &= \sum_{i=1}^{n}(x_i-\bar{x})(x_i-\bar{x}) \\
&= \sum_{i=1}^{n}(x_i(x_i-\bar{x})-\bar{x}(x_i-\bar{x})) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\sum_{i=1}^{n}\bar{x}(x_i - \bar{x}) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}\sum_{i=1}^{n}(x_i - \bar{x}) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}\bar{x}) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(\sum_{i=1}^{n}x_i - \bar{x}\sum_{i=1}^{n}(1)) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(\sum_{i=1}^{n}x_i - n\bar{x}) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(\sum_{i=1}^{n}x_i - n \frac{\sum_{i=1}^{n}x_i}{n}) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(\sum_{i=1}^{n}x_i - \sum_{i=1}^{n}x_i) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})-\bar{x}(0) \\
&= \sum_{i=1}^{n}x_i(x_i-\bar{x})
\end{aligned}
$$
De donde claramente tenemos que
$$
\boxed{
\begin{aligned}
\beta_1 &= \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \\
\beta_0 &= \bar{y}-\beta_1\bar{x}
\end{aligned}}
$$
que justamente son las fórmulas que queríamos encontrar. $\square$

## Referencias
- [Derivation of simple linear regression formula - Ivanky site](https://ivanky.wordpress.com/2018/05/20/derivation-of-simple-linear-regression-formula/)
- Linear Regression Analysis - Theory and Computing. Xin Yan, Xiao Gang Su
